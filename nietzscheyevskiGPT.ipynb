{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guided by Andrej Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "# Thus Spoke Zarathustra concatenated with Dostoyevsky's 'The Idiot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tokenizer import TextProcessor\n",
    "\n",
    "tp = TextProcessor(\"TSZ_input.txt\")\n",
    "data = tp.text\n",
    "\n",
    "data_enc = tp.map_to_int(data)\n",
    "data_enc = tf.convert_to_tensor(data_enc)\n",
    "\n",
    "train_split = int(len(data) * 0.9)\n",
    "xtrain = data_enc[:train_split]\n",
    "val_data = data_enc[train_split:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `tensor([76 62 63 1 75 63 60 80])` actually contains 7 different training examples in it; lets assume that these char to int 'embeddings' represent the word 'Nietzsche' the different examples would be:\n",
    "  * `[76]` (aka. `['N']`) -> is likely proceeded by a `[62]`(or `['i']`)\n",
    "  * `[76 62]` (aka. `['Ni']`) -> is likely proceeded by a `[63]` (or `['e']`)\n",
    "  * ...\n",
    "  * `[76 62 63  1 75 63 60]` (aka. `['Nietzsch']`) -> is likely proceeded by `[83]` (or `['e']`)\n",
    "<br>\n",
    "<br>\n",
    "* transformer will never receive more than chunk_size tokens/inputs at a time\n",
    " The below operation is not only done for efficiency but also so the transformer is 'used' to seeing context of size 1..chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 8\n",
    "x = xtrain[:chunk_size]\n",
    "y = xtrain[1:chunk_size+1]\n",
    "\n",
    "for k in range(chunk_size):\n",
    "    context = x[:k+1]\n",
    "    target = y[k]\n",
    "    # print(f'context: {context} target: {target}')\n",
    "# print(tf.__version__)\n",
    "# tp.stoi['T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 8]),\n",
       " TensorShape([32, 8]),\n",
       " tensorflow.python.framework.ops.EagerTensor,\n",
       " tensorflow.python.framework.ops.EagerTensor)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(44)\n",
    "batch_size = 32\n",
    "# Chunksize = context length, number of tokens to be considered\n",
    "# chunk_size = 8\n",
    "\n",
    "# get training batch\n",
    "# tf.function because the global seed is set and operation seeds are not set\n",
    "# @tf.function\n",
    "# However, adding the tf.function decorator causes the following error:\n",
    "# OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
    "\n",
    "def get_batch(dat):\n",
    "    dat = xtrain if dat == 'train' else val_data\n",
    "    randint = tf.random.uniform(shape=(batch_size,), maxval=len(dat)-chunk_size, dtype=tf.int64)\n",
    "    x = tf.stack([dat[i:i+chunk_size] for i in randint])\n",
    "    y = tf.stack([dat[i+1:i+chunk_size+1] for i in randint])\n",
    "    # print(f\"randint: {randint}, randint.shape: {randint.shape}\")\n",
    "    return x,y\n",
    "\n",
    "xs, ys = get_batch('train')\n",
    "\n",
    "\"\"\"\n",
    "x=[1,60,69..10]\n",
    "y = [60,69,..1]\n",
    "input x[0,0:2]= [1,60] output is y[2] = 69\n",
    "\"\"\"\n",
    "\n",
    "print('========================')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for k in range(chunk_size):\n",
    "        context = xs[b,:k+1]\n",
    "        target = ys[b,k]\n",
    "        # print(f'context: {context} target: {target}')\n",
    "\n",
    "xs.shape,ys.shape, type(xs), type(ys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford n-gram LM [source](https://web.stanford.edu/~jurafsky/slp3/3.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 93)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.010245682671666145,\n",
       " 0.010266579687595367,\n",
       " 0.010572953149676323,\n",
       " 0.010884030722081661,\n",
       " 0.010892984457314014,\n",
       " 0.010577243752777576,\n",
       " 0.010940386913716793,\n",
       " 0.011271117255091667,\n",
       " 0.01028439961373806,\n",
       " 0.010864926502108574]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "# from keras import layers, backend\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "# import keras.backend as K\n",
    "# rc: https://www.youtube.com/watch?v=PaCmpygFfXo&t=182s -> Bigram model details by Karpathy\n",
    "\n",
    "class ngramLangModel(keras.layers.Layer):\n",
    "    def __init__(self, vocab_size):\n",
    "        # `super` call to inherit from keras.layers.Layer\n",
    "        super(ngramLangModel,self).__init__()\n",
    "\n",
    "        self.token_embedding = keras.layers.Embedding(vocab_size, vocab_size)\n",
    "    # build used in situations where weights depend on the shape of the input tensors\n",
    "    def build(self, input_shape):\n",
    "        # if subclassers need a \"state creation step\"\n",
    "        # This method is used to create weights that depend on input.shape\n",
    "        # __call__ will auto build the layer if it hasn't been built yet\n",
    "\n",
    "        pass\n",
    "# ?keras.layers.MultiHeadAttention\n",
    "    # !inference mode vs training mode\n",
    "    def call(self, xss, targets=None):\n",
    "        # Called in __call__, after build has been called?\n",
    "        # __call__ wraps call\n",
    "        # Preforms logic of applying the layer to the input tensors\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        logits = self.token_embedding(xss)\n",
    "        print(logits.shape)\n",
    "\n",
    "        # from logits to normalized probabilities\n",
    "        # CategoricalCrossentropy expects labels to be provided in a one hot rep, labels as ints use SparseCategoricalCrossentropy\n",
    "\n",
    "        \"\"\"\n",
    "        # Figure out dimensions\n",
    "        # the err thrown is \"EagerTensor object has no attribute 'reshape'.\"\n",
    "        # the above enabling of numpy behavior has fixed this\n",
    "        \"\"\"\n",
    "        # loss = tf.nn.softmax_cross_entropy_with_logits\n",
    "        # loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        B, T, C = logits.shape\n",
    "        logits = tf.reshape(logits, (B*T, C))\n",
    "        targets = tf.reshape(targets, (B*T))\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "        self.add_loss(loss)\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self, xss, max_new_toks):\n",
    "\n",
    "        for _ in range(max_new_toks):\n",
    "            logits, loss = self(xss)\n",
    "            logits = logits[:, -1, :]\n",
    "            # obtain probability of each token (in this case char)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            next_token = tf.random.categorical(probs, num_samples=1) #(B, 1)\n",
    "            # first dimension/axis is the time dimension\n",
    "            xss = tf.concat([xss, next_token], axis=1)\n",
    "\n",
    "        return xss\n",
    "\n",
    "    # The below func can just be used as a layer. Src: https://blog.codecentric.de/move-n-gram-extraction-into-your-keras-model\n",
    "    # def ngram_block(n, alphabet_size):\n",
    "    #     def wrapped(inputs):\n",
    "    #         layer = layers.Conv1D(1, n, use_bias=False, trainable=False)\n",
    "    #         x = layers.Reshape((-1, 1))(inputs)\n",
    "    #         x = layer(x)\n",
    "    #         kernel = np.power(alphabet_size, range(0, n),\n",
    "    #                           dtype=backend.floatx())\n",
    "    #         layer.set_weights([kernel.reshape(n, 1, 1)])\n",
    "    #         return layers.Reshape((-1,))(x)\n",
    "\n",
    "    #     return wrapped\n",
    "\n",
    "    def bigram(self, data):\n",
    "        return tf.convert_to_tensor(Lambda(lambda x: [x[:,:-1] + x[:,1:] * tp.vocab_size])(data))\n",
    "\n",
    "\n",
    "\n",
    "ngram = ngramLangModel(tp.vocab_size)\n",
    "\n",
    "ys.shape,xs.shape\n",
    "out,loss = ngram(xs,ys)\n",
    "init_xss = tf.zeros((1, 1), dtype=tf.int64)\n",
    "out = tf.nn.softmax(out)\n",
    "x = out[0].numpy().tolist()\n",
    "# tp.decode_mapping(x)\n",
    "# ngram.generate(init_xss, max_new_toks=10)\n",
    "# print(out)\n",
    "# print(loss.call(ys,out))\n",
    "# type(bgramd)\n",
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on gpu\n",
    "# with tf.device('/GPU:0'):\n",
    "#     model = get_model()\n",
    "#     model.compile()\n",
    "#     model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
