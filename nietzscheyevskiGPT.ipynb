{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training text is Thus Spoke Zarathustra concatenated with Dostoyevsky's 'The Idiot'\n",
    "# Guided by Andrej Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 17:01:35.027853: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-08 17:01:35.070042: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-08 17:01:35.903086: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-08 17:01:37.528905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-08 17:01:37.612709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-08 17:01:37.612847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-08 17:01:37.614821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-08 17:01:37.615001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-08 17:01:37.615043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-08 17:01:38.554098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-08 17:01:38.554378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-08 17:01:38.554401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-08 17:01:38.554473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-08 17:01:38.554522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7335 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Tuple, Optional, Union\n",
    "from tokenizer import TextProcessor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# warnings.simplefilter(\"ignore\")\n",
    "tp = TextProcessor(\"TSZ_input.txt\")\n",
    "data = tp.text\n",
    "\n",
    "data_encoded = tp.map_to_int(data)\n",
    "data_encoded = tf.convert_to_tensor(data_encoded)\n",
    "\n",
    "train_split = int(len(data) * 0.9)\n",
    "xtrain = data_encoded[:train_split]\n",
    "val_data = data_encoded[train_split:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `tensor([76 62 63 1 75 63 60 80 83])` actually contains 7 different training examples in it; lets assume that these char to int 'embeddings' represent the word 'Nietzsche' the different examples would be:\n",
    "  - `[76]` (aka. `['N']`) -> is likely proceeded by a `[62]`(or `['i']`)\n",
    "  - `[76 62]` (aka. `['Ni']`) -> is likely proceeded by a `[63]` (or `['e']`)\n",
    "  - ...\n",
    "  - `[76 62 63  1 75 63 60 80]` (aka. `['Nietzsch']`) -> is likely proceeded by `[83]` (or `['e']`)\n",
    "    <br>\n",
    "    <br>\n",
    "- transformer will never receive more than chunk_size tokens/inputs at a time\n",
    "  The below operation is not only done for efficiency but also so the transformer is 'used' to seeing context of size 1..chunk_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 8\n",
    "x = xtrain[:chunk_size]\n",
    "y = xtrain[1 : chunk_size + 1]\n",
    "\n",
    "# for k in range(chunk_size):\n",
    "#     context = x[: k + 1]\n",
    "#     target = y[k]\n",
    "# print(f'context: {context} target: {target}')\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 8]),\n",
       " TensorShape([32, 8]),\n",
       " tensorflow.python.framework.ops.EagerTensor,\n",
       " tensorflow.python.framework.ops.EagerTensor)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(44)\n",
    "batch_size = 32\n",
    "# Chunksize = context length, number of tokens to be considered\n",
    "# chunk_size = 8\n",
    "\n",
    "# get training batch\n",
    "# tf.function because the global seed is set and operation seeds are not set\n",
    "# @tf.function\n",
    "# However, adding the tf.function decorator causes the following error:\n",
    "# OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
    "\n",
    "\n",
    "def get_batch(dat: str) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    dat = xtrain if dat == \"train\" else val_data\n",
    "    randint = tf.random.uniform(\n",
    "        shape=(batch_size,), maxval=len(dat) - chunk_size, dtype=tf.int64\n",
    "    )\n",
    "    x = tf.stack([dat[i : i + chunk_size] for i in randint])\n",
    "    y = tf.stack([dat[i + 1 : i + chunk_size + 1] for i in randint])\n",
    "    # print(f\"randint: {randint}, randint.shape: {randint.shape}\")\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xs, ys = get_batch(\"train\")\n",
    "\n",
    "\"\"\"\n",
    "x=[1,60,69..10]\n",
    "y = [60,69,..1]\n",
    "input x[0,0:2]= [1,60] output is y[2] = 69\n",
    "\"\"\"\n",
    "\n",
    "# for b in range(batch_size):\n",
    "#     for k in range(chunk_size):\n",
    "#         context = xs[b, : k + 1]\n",
    "#         target = ys[b, k]\n",
    "# print(f\"context: {context} target: {target}\")\n",
    "\n",
    "xs.shape, ys.shape, type(xs), type(ys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford n-gram LM [source](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2325061986.py, line 133)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 133\u001b[0;36m\u001b[0m\n\u001b[0;31m    ctx = tf.zeros((1,1), dtype=tf.int64)))\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# from keras import layers, backend\n",
    "from keras.layers import Lambda\n",
    "\n",
    "# from tensorflow.python.ops.numpy_ops import np_config\n",
    "# np_config.enable_numpy_behavior()\n",
    "# BATCH, TIME, CONTEXT\n",
    "\n",
    "\n",
    "# import keras.backend as K\n",
    "class NgramLangModel(keras.layers.Layer):\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = keras.layers.Dropout(0.1)\n",
    "        self.logits = keras.layers.Dense(vocab_size, activation=None)\n",
    "        self.token_embedding = keras.layers.Embedding(vocab_size, vocab_size)\n",
    "        self.ff = keras.layers.Dense(vocab_size, activation=None)\n",
    "        self.loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # build used in situations where weights depend on the shape of the input tensors\n",
    "    # def build(self, input_shape):\n",
    "    #     if subclassers need a \"state creation step\"\n",
    "    #     This method is used to create weights that depend on input.shape\n",
    "    #     __call__ will auto build the layer if it hasn't been built yet\n",
    "\n",
    "    # ?keras.layers.MultiHeadAttention\n",
    "    # !inference mode vs training mode\n",
    "    # Called in __call__, after build has been called?\n",
    "    # __call__ wraps call\n",
    "    # Preforms logic of applying the layer to the input tensors\n",
    "    # from logits to normalized probabilities\n",
    "    # CategoricalCrossentropy expects labels to be provided in a one hot rep, labels as ints use SparseCategoricalCrossentropy\n",
    "    # logits = self.token_embedding(xss)\n",
    "    # # print(f\"logits.shape: {logits.shape}\")\n",
    "    # if targets is None:\n",
    "    #     loss = None\n",
    "    # else:\n",
    "    #     B, T, C = logits.shape\n",
    "    #     logits = tf.reshape(logits, (B * T, C))\n",
    "    #     targets = tf.reshape(targets, (B * T))\n",
    "    #     loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    #         logits=logits, labels=targets\n",
    "    #     )\n",
    "    #     self.add_loss(loss)\n",
    "    #     return logits, loss\n",
    "\n",
    "    def call(\n",
    "        self, xss: tf.Tensor, targets: Optional[tf.Tensor] = None, training: bool = True\n",
    "    ) -> Union[tf.Tensor, Tuple[tf.Tensor, tf.Tensor]]:\n",
    "        xs = self.token_embedding(xss)  # embedded input\n",
    "        xs = self.dropout(xs, training=training)  # dropout\n",
    "        xs = self.ff(xs)  # pass through feed-forward\n",
    "        logits = self.logits(xs)  # output layer\n",
    "        # print(f\"logits.shape: {logits.shape}\")\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = self.loss_fn(targets, logits)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits\n",
    "        # loss = tf.nn.softmax_cross_entropy_with_logits\n",
    "        # loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    def generate(self, xss: tf.Tensor, max_new_toks: int = 128) -> tf.Tensor:\n",
    "        for _ in range(max_new_toks):\n",
    "            res = self(xss)  # No targets\n",
    "            if isinstance(res, tuple):\n",
    "                logits, loss = res\n",
    "            else:\n",
    "                logits = res\n",
    "            logits = logits[:, -1, :]\n",
    "            # obtain probability of each token (in this case char)\n",
    "            logits = tf.nn.softmax(logits)\n",
    "\n",
    "            next_token = tf.random.categorical(logits, num_samples=1)  # (B, 1)\n",
    "            # first dimension/axis is the time dimension\n",
    "            xss = tf.concat([xss, next_token], axis=1)\n",
    "            \"\"\"\n",
    "            xss: current context of some chars in a batch of dimensions (B, T)\n",
    "            generates (B, T+1)...(B, T+max_new_toks)\n",
    "            \"\"\"\n",
    "        return xss\n",
    "\n",
    "    # The below func can just be used as a layer. Src: https://blog.codecentric.de/move-n-gram-extraction-into-your-keras-ngram\n",
    "    # def ngram_block(n, alphabet_size):\n",
    "    #     def wrapped(inputs):\n",
    "    #         layer = layers.Conv1D(1, n, use_bias=False, trainable=False)\n",
    "    #         x = layers.Reshape((-1, 1))(inputs)\n",
    "    #         x = layer(x)\n",
    "    #         kernel = np.power(alphabet_size, range(0, n),\n",
    "    #                           dtype=backend.floatx())\n",
    "    #         layer.set_weights([kernel.reshape(n, 1, 1)])\n",
    "    #         return layers.Reshape((-1,))(x)\n",
    "\n",
    "    #     return wrapped\n",
    "\n",
    "    def bigram(self, data):\n",
    "        return tf.convert_to_tensor(\n",
    "            Lambda(lambda x: [x[:, :-1] + x[:, 1:] * tp.vocab_size])(data)\n",
    "        )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "model.eval() in torch -> training=False in tf\n",
    "\"\"\"\n",
    "\n",
    "ngram = NgramLangModel(tp.vocab_size)\n",
    "\n",
    "\n",
    "ngram.call(x, y)\n",
    "\n",
    "\n",
    "@tf.no_gradient(op_type=\"*\")\n",
    "def calculate_loss() -> dict:\n",
    "    eval_iters = 400\n",
    "    out = {}\n",
    "    # with tf.GradientTape() as _:\n",
    "    for split in [\"train\", \"eval\"]:\n",
    "        losses = []\n",
    "        for _ in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = ngram(X, Y)\n",
    "            losses.append(loss)\n",
    "        out[split] = tf.reduce_mean(losses)\n",
    "    return out\n",
    "\n",
    "\n",
    "def loss_fn() -> tf.Tensor:\n",
    "    X, Y = get_batch(\"train\")\n",
    "    _, loss = ngram(X, Y, training=True)\n",
    "    return loss\n",
    "ctx = tf.zeros((1,1), dtype=tf.int64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tp.decode_mapping(ngram.generate(ctx)[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 0: training loss: 2.5764, eval loss: 2.5907\n",
      "timestep: 100: training loss: 2.5716, eval loss: 2.5807\n",
      "timestep: 200: training loss: 2.5532, eval loss: 2.5658\n",
      "timestep: 300: training loss: 2.5558, eval loss: 2.5540\n",
      "timestep: 400: training loss: 2.5387, eval loss: 2.5483\n",
      "timestep: 500: training loss: 2.5459, eval loss: 2.5613\n",
      "timestep: 600: training loss: 2.5407, eval loss: 2.5563\n",
      "timestep: 700: training loss: 2.5426, eval loss: 2.5475\n",
      "timestep: 800: training loss: 2.5383, eval loss: 2.5502\n",
      "timestep: 900: training loss: 2.5356, eval loss: 2.5468\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=$CONDA_PREFIX_1/pkgs/cuda-nvcc-11.7.99-0\"\n",
    "\n",
    "\n",
    "optimiser = keras.optimizers.AdamW(learning_rate=1e-3)\n",
    "for timestep in range(1000):\n",
    "    if timestep % 100 == 0:\n",
    "        loss = calculate_loss()\n",
    "        print(\n",
    "            f\"timestep: {timestep}: training loss: {loss['train']:.4f}, eval loss: {loss['eval']:.4f}\"\n",
    "        )\n",
    "    optimiser.minimize(loss_fn, ngram.trainable_variables)\n",
    "\n",
    "context = tf.zeros((1, 1), dtype=tf.int64)\n",
    "# print(tp.decode_mapping(ngram.generate(context, max_new_toks=60)[0].numpy().tolist()))\n",
    "# ys.shape, xs.shape\n",
    "# out, loss = ngram(xs, ys)\n",
    "# init_xss = tf.zeros((1, 1), dtype=tf.int64)\n",
    "# out = tf.nn.softmax(out)\n",
    "# x = out[0].numpy().tolist()\n",
    "# tp.decode_mapping(x)\n",
    "# ngram.generate(init_xss, max_new_toks=10)\n",
    "# print(out)\n",
    "# print(loss.call(ys,out))\n",
    "# x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f√®b5√®n‚Äù‚Äù$YW\n",
      "esZk09t!'jyRJO/%gZCMp5I_d]/Jk‚ÄùicX‚Äúy √®I!0-gQ:6/*JPsmsY‚ÄôNM,yh8$AH√©VOLUsOxM‚Äújl0z,)-l‚Äò TM:NEDB‚ÄùM‚ÄîX_aB'65'L$SeCZq:n:√†rABY\n"
     ]
    }
   ],
   "source": [
    "print(tp.decode_mapping(ngram.generate(context)[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "h = tf.random.normal(shape=(B, T, C), dtype=tf.float32)\n",
    "\"\"\"\n",
    "for every batch element, for every t'th token we want to avg\n",
    "all the tokens from 0 to t-1 (inclusive)\n",
    "keep in mind this is very lossy, we are throwing away a lot of information\n",
    "such as positional information\n",
    "\"\"\"\n",
    "bow_lst = []\n",
    "# xbagOfWords = tf.zeros((B, T, C))\n",
    "\n",
    "# ?cannot assign to eager tensor\n",
    "for b in range(B):\n",
    "    batch_list = []\n",
    "    for t in range(T):\n",
    "        # xbagOfWords[b, t] = tf.reduce_mean(h[b, : t + 1], axis=0)\n",
    "        # mean = tf.reduce_mean(h[b, : t + 1], axis=0)\n",
    "        # indexing h[b, : t + 1] will change shape to (t,C)\n",
    "        batch_list.append(tf.reduce_mean(h[b, : t + 1], axis=0))\n",
    "    bow_lst.append(batch_list)\n",
    "xbagOfWords = tf.stack(bow_lst)\n",
    "assert all(xbagOfWords[0][-1] == tf.reduce_mean(h[0, :8], axis=0))\n",
    "# since we are just averaging and the last element in BOW is the average of all prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATMUL TIME FOR EFFICIENCY üòÑ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
