{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training text is Thus Spoke Zarathustra concatenated with Dostoyevsky's 'The Idiot'\n",
    "# Guided by Andrej Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# import numpy as np\n",
    "from tokenizer import TextProcessor\n",
    "\n",
    "tp = TextProcessor(\"TSZ_input.txt\")\n",
    "data = tp.text\n",
    "\n",
    "data_enc = tp.map_to_int(data)\n",
    "data_enc = tf.convert_to_tensor(data_enc)\n",
    "\n",
    "train_split = int(len(data) * 0.9)\n",
    "xtrain = data_enc[:train_split]\n",
    "val_data = data_enc[train_split:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `tensor([76 62 63 1 75 63 60 80 83])` actually contains 7 different training examples in it; lets assume that these char to int 'embeddings' represent the word 'Nietzsche' the different examples would be:\n",
    "  - `[76]` (aka. `['N']`) -> is likely proceeded by a `[62]`(or `['i']`)\n",
    "  - `[76 62]` (aka. `['Ni']`) -> is likely proceeded by a `[63]` (or `['e']`)\n",
    "  - ...\n",
    "  - `[76 62 63  1 75 63 60 80]` (aka. `['Nietzsch']`) -> is likely proceeded by `[83]` (or `['e']`)\n",
    "    <br>\n",
    "    <br>\n",
    "- transformer will never receive more than chunk_size tokens/inputs at a time\n",
    "  The below operation is not only done for efficiency but also so the transformer is 'used' to seeing context of size 1..chunk_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 8\n",
    "x = xtrain[:chunk_size]\n",
    "y = xtrain[1 : chunk_size + 1]\n",
    "\n",
    "for k in range(chunk_size):\n",
    "    context = x[: k + 1]\n",
    "    target = y[k]\n",
    "    # print(f'context: {context} target: {target}')\n",
    "# print(tf.__version__)\n",
    "# tp.stoi['T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 8]),\n",
       " TensorShape([32, 8]),\n",
       " tensorflow.python.framework.ops.EagerTensor,\n",
       " tensorflow.python.framework.ops.EagerTensor)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(44)\n",
    "batch_size = 32\n",
    "# Chunksize = context length, number of tokens to be considered\n",
    "# chunk_size = 8\n",
    "\n",
    "# get training batch\n",
    "# tf.function because the global seed is set and operation seeds are not set\n",
    "# @tf.function\n",
    "# However, adding the tf.function decorator causes the following error:\n",
    "# OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
    "\n",
    "\n",
    "def get_batch(dat):\n",
    "    dat = xtrain if dat == \"train\" else val_data\n",
    "    randint = tf.random.uniform(\n",
    "        shape=(batch_size,), maxval=len(dat) - chunk_size, dtype=tf.int64\n",
    "    )\n",
    "    x = tf.stack([dat[i : i + chunk_size] for i in randint])\n",
    "    y = tf.stack([dat[i + 1 : i + chunk_size + 1] for i in randint])\n",
    "    # print(f\"randint: {randint}, randint.shape: {randint.shape}\")\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xs, ys = get_batch(\"train\")\n",
    "\n",
    "\"\"\"\n",
    "x=[1,60,69..10]\n",
    "y = [60,69,..1]\n",
    "input x[0,0:2]= [1,60] output is y[2] = 69\n",
    "\"\"\"\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for k in range(chunk_size):\n",
    "        context = xs[b, : k + 1]\n",
    "        target = ys[b, k]\n",
    "        # print(f'context: {context} target: {target}')\n",
    "\n",
    "xs.shape, ys.shape, type(xs), type(ys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford n-gram LM [source](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 0: training loss: 4.5365, eval loss: 4.5365\n",
      "timestep: 100: training loss: 2.8236, eval loss: 2.8186\n",
      "timestep: 200: training loss: 2.6436, eval loss: 2.6506\n",
      "timestep: 300: training loss: 2.5936, eval loss: 2.6039\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "# from keras import layers, backend\n",
    "from keras.layers import Lambda\n",
    "\n",
    "# from tensorflow.python.ops.numpy_ops import np_config\n",
    "# np_config.enable_numpy_behavior()\n",
    "# BATCH, TIME, CONTEXT\n",
    "\n",
    "\n",
    "# import keras.backend as K\n",
    "class ngramLangModel(keras.layers.Layer):\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        # `super` call to inherit from keras.layers.Layer\n",
    "        super().__init__()\n",
    "        self.dropout = keras.layers.Dropout(0.1)\n",
    "\n",
    "        self.logits = keras.layers.Dense(vocab_size, activation=None)\n",
    "        self.token_embedding = keras.layers.Embedding(vocab_size, vocab_size)\n",
    "        self.ff = keras.layers.Dense(vocab_size, activation=None)  # Feedforward layer\n",
    "\n",
    "    # build used in situations where weights depend on the shape of the input tensors\n",
    "    # def build(self, input_shape):\n",
    "    #     if subclassers need a \"state creation step\"\n",
    "    #     This method is used to create weights that depend on input.shape\n",
    "    #     __call__ will auto build the layer if it hasn't been built yet\n",
    "\n",
    "    # ?keras.layers.MultiHeadAttention\n",
    "    # !inference mode vs training mode\n",
    "    # Called in __call__, after build has been called?\n",
    "    # __call__ wraps call\n",
    "    # Preforms logic of applying the layer to the input tensors\n",
    "    # from logits to normalized probabilities\n",
    "    # CategoricalCrossentropy expects labels to be provided in a one hot rep, labels as ints use SparseCategoricalCrossentropy\n",
    "    # logits = self.token_embedding(xss)\n",
    "    # # print(f\"logits.shape: {logits.shape}\")\n",
    "    # if targets is None:\n",
    "    #     loss = None\n",
    "    # else:\n",
    "    #     B, T, C = logits.shape\n",
    "    #     logits = tf.reshape(logits, (B * T, C))\n",
    "    #     targets = tf.reshape(targets, (B * T))\n",
    "    #     loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    #         logits=logits, labels=targets\n",
    "    #     )\n",
    "    #     self.add_loss(loss)\n",
    "    #     return logits, loss\n",
    "\n",
    "    # todo: Fix typo in call docs tf PR:\n",
    "    def call(\n",
    "        self, xss: tf.Tensor, targets: Optional[tf.Tensor] = None, training: bool = True\n",
    "    ) -> Union[tf.Tensor, Tuple[tf.Tensor, tf.Tensor]]:\n",
    "        xs = self.token_embedding(xss)  # embedded input\n",
    "        xs = self.dropout(xs, training=training)  # dropout\n",
    "        xs = self.ff(xs)  # pass through feed-forward\n",
    "        logits = self.logits(xs)  # output layer\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = keras.losses.sparse_categorical_crossentropy(\n",
    "                targets, logits, from_logits=True\n",
    "            )\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits\n",
    "        # loss = tf.nn.softmax_cross_entropy_with_logits\n",
    "        # loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    def generate(self, xss: tf.Tensor, max_new_toks: int) -> tf.Tensor:\n",
    "        for _ in range(max_new_toks):\n",
    "            res = self(xss)\n",
    "            if isinstance(res, tuple):\n",
    "                logits, loss = res\n",
    "            else:\n",
    "                logits = res\n",
    "            logits = logits[:, -1, :]\n",
    "            # obtain probability of each token (in this case char)\n",
    "            # probs = tf.nn.softmax(logits)\n",
    "            next_token = tf.random.categorical(logits, num_samples=1)  # (B, 1)\n",
    "            # first dimension/axis is the time dimension\n",
    "            xss = tf.concat([xss, next_token], axis=1)\n",
    "        return xss\n",
    "\n",
    "    # The below func can just be used as a layer. Src: https://blog.codecentric.de/move-n-gram-extraction-into-your-keras-ngram\n",
    "    # def ngram_block(n, alphabet_size):\n",
    "    #     def wrapped(inputs):\n",
    "    #         layer = layers.Conv1D(1, n, use_bias=False, trainable=False)\n",
    "    #         x = layers.Reshape((-1, 1))(inputs)\n",
    "    #         x = layer(x)\n",
    "    #         kernel = np.power(alphabet_size, range(0, n),\n",
    "    #                           dtype=backend.floatx())\n",
    "    #         layer.set_weights([kernel.reshape(n, 1, 1)])\n",
    "    #         return layers.Reshape((-1,))(x)\n",
    "\n",
    "    #     return wrapped\n",
    "\n",
    "    def bigram(self, data):\n",
    "        return tf.convert_to_tensor(\n",
    "            Lambda(lambda x: [x[:, :-1] + x[:, 1:] * tp.vocab_size])(data)\n",
    "        )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "model.eval() in torch -> training=False in tf\n",
    "\"\"\"\n",
    "\n",
    "ngram = ngramLangModel(tp.vocab_size)\n",
    "\n",
    "\n",
    "# @tf.no_gradient(op_type=\"*\")\n",
    "def calculate_loss() -> dict:\n",
    "    eval_iters = 400\n",
    "    out = {}\n",
    "    # with tf.GradientTape() as _:\n",
    "    for split in [\"train\", \"eval\"]:\n",
    "        losses = []\n",
    "        for _ in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = ngram(X, Y)\n",
    "            losses.append(loss.numpy())\n",
    "        out[split] = tf.reduce_mean(losses)\n",
    "    return out\n",
    "\n",
    "\n",
    "def loss_fn() -> tf.Tensor:\n",
    "    X, Y = get_batch(\"train\")\n",
    "    _, loss = ngram(X, Y, training=True)\n",
    "    return loss\n",
    "\n",
    "\n",
    "optimiser = keras.optimizers.AdamW(learning_rate=1e-3)\n",
    "\n",
    "for timestep in range(400):\n",
    "    if timestep % 100 == 0:\n",
    "        loss = calculate_loss()\n",
    "        print(\n",
    "            f\"timestep: {timestep}: training loss: {loss['train']:.4f}, eval loss: {loss['eval']:.4f}\"\n",
    "        )\n",
    "    optimiser.minimize(loss_fn, ngram.trainable_variables)\n",
    "\n",
    "context = tf.zeros((1, 1), dtype=tf.int64)\n",
    "# print(tp.decode_mapping(ngram.generate(context, max_new_toks=60)[0].numpy().tolist()))\n",
    "# ys.shape, xs.shape\n",
    "# out, loss = ngram(xs, ys)\n",
    "# init_xss = tf.zeros((1, 1), dtype=tf.int64)\n",
    "# out = tf.nn.softmax(out)\n",
    "# x = out[0].numpy().tolist()\n",
    "# tp.decode_mapping(x)\n",
    "# ngram.generate(init_xss, max_new_toks=10)\n",
    "# print(out)\n",
    "# print(loss.call(ys,out))\n",
    "# x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "andicowhig thi!\n",
      "\n",
      "ce bel—mof fuedingusanore\n",
      "If locuth s be!”\n",
      "\n",
      "I Hersis r, wfobeng amll\n",
      "m ced ag wsuto\n"
     ]
    }
   ],
   "source": [
    "print(tp.decode_mapping(ngram.generate(context, max_new_toks=100)[0].numpy().tolist()))\n",
    "# train on gpu\n",
    "# with tf.device('/GPU:0'):\n",
    "#     model = get_model()\n",
    "#     model.compile()\n",
    "#     model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
