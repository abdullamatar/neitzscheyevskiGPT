{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guided by Andrej Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "# Thus Spoke Zarathustra concatenated with Dostoyevsky's 'The Idiot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tokenizer import TextProcessor\n",
    "\n",
    "tp = TextProcessor(\"TSZ_input.txt\")\n",
    "data = tp.text\n",
    "\n",
    "data_enc = tp.map_to_int(data)\n",
    "data_enc = np.array(data_enc)\n",
    "\n",
    "train_split = int(len(data) * 0.9)\n",
    "xtrain = data_enc[:train_split]\n",
    "val_data = data_enc[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 8\n",
    "x = xtrain[:chunk_size]\n",
    "y = xtrain[1:chunk_size+1]\n",
    "\n",
    "# transformer will never receive more than chunk_size tokens/inputs at a time\n",
    "# The below operation is not only done for efficiency but also so the transformer is 'used' to seeing context of size 1..chunk_size\n",
    "for k in range(chunk_size):\n",
    "    context = x[:k+1]\n",
    "    target = y[k]\n",
    "    # print(f'context: {context} target: {target}')\n",
    "# print(tf.__version__)\n",
    "# tp.stoi['T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randint: [ 440729  233150  255930 1737213   84626  586070 1703100 1721474 1718721\n",
      "  827026 1317033 1378346  416373 1223467 1700808  817080  137852 1298681\n",
      "  803705 1431576  142775  151798 1691730 1798182 1784864  330892 1206681\n",
      " 1743925  871660  670485  110458  940298], randint.shape: (32,)\n",
      "========================\n",
      "context: [73] target: 59\n",
      "context: [73 59] target: 10\n",
      "context: [73 59 10] target: 1\n",
      "context: [73 59 10  1] target: 56\n",
      "context: [73 59 10  1 56] target: 67\n",
      "context: [73 59 10  1 56 67] target: 67\n",
      "context: [73 59 10  1 56 67 67] target: 1\n",
      "context: [73 59 10  1 56 67 67  1] target: 62\n",
      "context: [58] target: 70\n",
      "context: [58 70] target: 73\n",
      "context: [58 70 73] target: 73\n",
      "context: [58 70 73 73] target: 64\n",
      "context: [58 70 73 73 64] target: 59\n",
      "context: [58 70 73 73 64 59] target: 70\n",
      "context: [58 70 73 73 64 59 70] target: 73\n",
      "context: [58 70 73 73 64 59 70 73] target: 74\n",
      "context: [59] target: 74\n",
      "context: [59 74] target: 12\n",
      "context: [59 74 12] target: 0\n",
      "context: [59 74 12  0] target: 0\n",
      "context: [59 74 12  0  0] target: 0\n",
      "context: [59 74 12  0  0  0] target: 0\n",
      "context: [59 74 12  0  0  0  0] target: 0\n",
      "context: [59 74 12  0  0  0  0  0] target: 0\n",
      "context: [78] target: 63\n",
      "context: [78 63] target: 56\n",
      "context: [78 63 56] target: 75\n",
      "context: [78 63 56 75] target: 1\n",
      "context: [78 63 56 75  1] target: 59\n",
      "context: [78 63 56 75  1 59] target: 70\n",
      "context: [78 63 56 75  1 59 70] target: 1\n",
      "context: [78 63 56 75  1 59 70  1] target: 80\n",
      "context: [76] target: 62\n",
      "context: [76 62] target: 63\n",
      "context: [76 62 63] target: 1\n",
      "context: [76 62 63  1] target: 75\n",
      "context: [76 62 63  1 75] target: 63\n",
      "context: [76 62 63  1 75 63] target: 60\n",
      "context: [76 62 63  1 75 63 60] target: 80\n",
      "context: [76 62 63  1 75 63 60 80] target: 1\n",
      "context: [60] target: 1\n",
      "context: [60  1] target: 76\n",
      "context: [60  1 76] target: 67\n",
      "context: [60  1 76 67] target: 75\n",
      "context: [60  1 76 67 75] target: 64\n",
      "context: [60  1 76 67 75 64] target: 68\n",
      "context: [60  1 76 67 75 64 68] target: 56\n",
      "context: [60  1 76 67 75 64 68 56] target: 75\n",
      "context: [70] target: 73\n",
      "context: [70 73] target: 1\n",
      "context: [70 73  1] target: 70\n",
      "context: [70 73  1 70] target: 69\n",
      "context: [70 73  1 70 69] target: 60\n",
      "context: [70 73  1 70 69 60] target: 1\n",
      "context: [70 73  1 70 69 60  1] target: 74\n",
      "context: [70 73  1 70 69 60  1 74] target: 60\n",
      "context: [75] target: 63\n",
      "context: [75 63] target: 64\n",
      "context: [75 63 64] target: 69\n",
      "context: [75 63 64 69] target: 66\n",
      "context: [75 63 64 69 66] target: 0\n",
      "context: [75 63 64 69 66  0] target: 74\n",
      "context: [75 63 64 69 66  0 74] target: 70\n",
      "context: [75 63 64 69 66  0 74 70] target: 26\n",
      "context: [74] target: 1\n",
      "context: [74  1] target: 57\n",
      "context: [74  1 57] target: 76\n",
      "context: [74  1 57 76] target: 73\n",
      "context: [74  1 57 76 73] target: 64\n",
      "context: [74  1 57 76 73 64] target: 60\n",
      "context: [74  1 57 76 73 64 60] target: 59\n",
      "context: [74  1 57 76 73 64 60 59] target: 1\n",
      "context: [60] target: 73\n",
      "context: [60 73] target: 26\n",
      "context: [60 73 26] target: 92\n",
      "context: [60 73 26 92] target: 1\n",
      "context: [60 73 26 92  1] target: 56\n",
      "context: [60 73 26 92  1 56] target: 74\n",
      "context: [60 73 26 92  1 56 74] target: 66\n",
      "context: [60 73 26 92  1 56 74 66] target: 60\n",
      "context: [64] target: 69\n",
      "context: [64 69] target: 58\n",
      "context: [64 69 58] target: 60\n",
      "context: [64 69 58 60] target: 12\n",
      "context: [64 69 58 60 12] target: 1\n",
      "context: [64 69 58 60 12  1] target: 35\n",
      "context: [64 69 58 60 12  1 35] target: 1\n",
      "context: [64 69 58 60 12  1 35  1] target: 73\n",
      "context: [77] target: 67\n",
      "context: [77 67] target: 70\n",
      "context: [77 67 70] target: 77\n",
      "context: [77 67 70 77] target: 64\n",
      "context: [77 67 70 77 64] target: 75\n",
      "context: [77 67 70 77 64 75] target: 58\n",
      "context: [77 67 70 77 64 75 58] target: 63\n",
      "context: [77 67 70 77 64 75 58 63] target: 25\n",
      "context: [74] target: 1\n",
      "context: [74  1] target: 63\n",
      "context: [74  1 63] target: 60\n",
      "context: [74  1 63 60] target: 56\n",
      "context: [74  1 63 60 56] target: 73\n",
      "context: [74  1 63 60 56 73] target: 75\n",
      "context: [74  1 63 60 56 73 75] target: 10\n",
      "context: [74  1 63 60 56 73 75 10] target: 1\n",
      "context: [1] target: 69\n",
      "context: [ 1 69] target: 70\n",
      "context: [ 1 69 70] target: 1\n",
      "context: [ 1 69 70  1] target: 68\n",
      "context: [ 1 69 70  1 68] target: 70\n",
      "context: [ 1 69 70  1 68 70] target: 73\n",
      "context: [ 1 69 70  1 68 70 73] target: 60\n",
      "context: [ 1 69 70  1 68 70 73 60] target: 1\n",
      "context: [1] target: 57\n",
      "context: [ 1 57] target: 60\n",
      "context: [ 1 57 60] target: 1\n",
      "context: [ 1 57 60  1] target: 63\n",
      "context: [ 1 57 60  1 63] target: 60\n",
      "context: [ 1 57 60  1 63 60] target: 56\n",
      "context: [ 1 57 60  1 63 60 56] target: 73\n",
      "context: [ 1 57 60  1 63 60 56 73] target: 59\n",
      "context: [73] target: 80\n",
      "context: [73 80] target: 12\n",
      "context: [73 80 12] target: 1\n",
      "context: [73 80 12  1] target: 46\n",
      "context: [73 80 12  1 46] target: 63\n",
      "context: [73 80 12  1 46 63] target: 60\n",
      "context: [73 80 12  1 46 63 60] target: 73\n",
      "context: [73 80 12  1 46 63 60 73] target: 60\n",
      "context: [60] target: 73\n",
      "context: [60 73] target: 80\n",
      "context: [60 73 80] target: 1\n",
      "context: [60 73 80  1] target: 69\n",
      "context: [60 73 80  1 69] target: 60\n",
      "context: [60 73 80  1 69 60] target: 60\n",
      "context: [60 73 80  1 69 60 60] target: 59\n",
      "context: [60 73 80  1 69 60 60 59] target: 1\n",
      "context: [73] target: 60\n",
      "context: [73 60] target: 59\n",
      "context: [73 60 59] target: 1\n",
      "context: [73 60 59  1] target: 70\n",
      "context: [73 60 59  1 70] target: 69\n",
      "context: [73 60 59  1 70 69] target: 1\n",
      "context: [73 60 59  1 70 69  1] target: 75\n",
      "context: [73 60 59  1 70 69  1 75] target: 63\n",
      "context: [76] target: 58\n",
      "context: [76 58] target: 63\n",
      "context: [76 58 63] target: 1\n",
      "context: [76 58 63  1] target: 56\n",
      "context: [76 58 63  1 56] target: 74\n",
      "context: [76 58 63  1 56 74] target: 1\n",
      "context: [76 58 63  1 56 74  1] target: 76\n",
      "context: [76 58 63  1 56 74  1 76] target: 69\n",
      "context: [59] target: 70\n",
      "context: [59 70] target: 78\n",
      "context: [59 70 78] target: 69\n",
      "context: [59 70 78 69] target: 1\n",
      "context: [59 70 78 69  1] target: 57\n",
      "context: [59 70 78 69  1 57] target: 60\n",
      "context: [59 70 78 69  1 57 60] target: 74\n",
      "context: [59 70 78 69  1 57 60 74] target: 64\n",
      "context: [1] target: 78\n",
      "context: [ 1 78] target: 56\n",
      "context: [ 1 78 56] target: 64\n",
      "context: [ 1 78 56 64] target: 75\n",
      "context: [ 1 78 56 64 75] target: 64\n",
      "context: [ 1 78 56 64 75 64] target: 69\n",
      "context: [ 1 78 56 64 75 64 69] target: 62\n",
      "context: [ 1 78 56 64 75 64 69 62] target: 1\n",
      "context: [74] target: 75\n",
      "context: [74 75] target: 73\n",
      "context: [74 75 73] target: 56\n",
      "context: [74 75 73 56] target: 2\n",
      "context: [74 75 73 56  2] target: 1\n",
      "context: [74 75 73 56  2  1] target: 49\n",
      "context: [74 75 73 56  2  1 49] target: 56\n",
      "context: [74 75 73 56  2  1 49 56] target: 67\n",
      "context: [48] target: 56\n",
      "context: [48 56] target: 73\n",
      "context: [48 56 73] target: 64\n",
      "context: [48 56 73 64] target: 56\n",
      "context: [48 56 73 64 56] target: 12\n",
      "context: [48 56 73 64 56 12] target: 1\n",
      "context: [48 56 73 64 56 12  1] target: 33\n",
      "context: [48 56 73 64 56 12  1 33] target: 56\n",
      "context: [77] target: 60\n",
      "context: [77 60] target: 73\n",
      "context: [77 60 73] target: 56\n",
      "context: [77 60 73 56] target: 67\n",
      "context: [77 60 73 56 67] target: 1\n",
      "context: [77 60 73 56 67  1] target: 69\n",
      "context: [77 60 73 56 67  1 69] target: 64\n",
      "context: [77 60 73 56 67  1 69 64] target: 62\n",
      "context: [1] target: 34\n",
      "context: [ 1 34] target: 60\n",
      "context: [ 1 34 60] target: 1\n",
      "context: [ 1 34 60  1] target: 63\n",
      "context: [ 1 34 60  1 63] target: 56\n",
      "context: [ 1 34 60  1 63 56] target: 59\n",
      "context: [ 1 34 60  1 63 56 59] target: 1\n",
      "context: [ 1 34 60  1 63 56 59  1] target: 75\n",
      "context: [74] target: 56\n",
      "context: [74 56] target: 64\n",
      "context: [74 56 64] target: 67\n",
      "context: [74 56 64 67] target: 70\n",
      "context: [74 56 64 67 70] target: 73\n",
      "context: [74 56 64 67 70 73] target: 74\n",
      "context: [74 56 64 67 70 73 74] target: 1\n",
      "context: [74 56 64 67 70 73 74  1] target: 56\n",
      "context: [1] target: 7\n",
      "context: [1 7] target: 55\n",
      "context: [ 1  7 55] target: 30\n",
      "context: [ 1  7 55 30] target: 60\n",
      "context: [ 1  7 55 30 60] target: 1\n",
      "context: [ 1  7 55 30 60  1] target: 71\n",
      "context: [ 1  7 55 30 60  1 71] target: 73\n",
      "context: [ 1  7 55 30 60  1 71 73] target: 70\n",
      "context: [59] target: 0\n",
      "context: [59  0] target: 75\n",
      "context: [59  0 75] target: 63\n",
      "context: [59  0 75 63] target: 56\n",
      "context: [59  0 75 63 56] target: 75\n",
      "context: [59  0 75 63 56 75] target: 10\n",
      "context: [59  0 75 63 56 75 10] target: 1\n",
      "context: [59  0 75 63 56 75 10  1] target: 29\n",
      "context: [60] target: 78\n",
      "context: [60 78] target: 1\n",
      "context: [60 78  1] target: 70\n",
      "context: [60 78  1 70] target: 61\n",
      "context: [60 78  1 70 61] target: 61\n",
      "context: [60 78  1 70 61 61] target: 1\n",
      "context: [60 78  1 70 61 61  1] target: 63\n",
      "context: [60 78  1 70 61 61  1 63] target: 60\n",
      "context: [78] target: 56\n",
      "context: [78 56] target: 74\n",
      "context: [78 56 74] target: 1\n",
      "context: [78 56 74  1] target: 78\n",
      "context: [78 56 74  1 78] target: 56\n",
      "context: [78 56 74  1 78 56] target: 67\n",
      "context: [78 56 74  1 78 56 67] target: 66\n",
      "context: [78 56 74  1 78 56 67 66] target: 64\n",
      "context: [60] target: 74\n",
      "context: [60 74] target: 2\n",
      "context: [60 74  2] target: 0\n",
      "context: [60 74  2  0] target: 0\n",
      "context: [60 74  2  0  0] target: 29\n",
      "context: [60 74  2  0  0 29] target: 63\n",
      "context: [60 74  2  0  0 29 63] target: 56\n",
      "context: [60 74  2  0  0 29 63 56] target: 69\n",
      "context: [70] target: 70\n",
      "context: [70 70] target: 59\n",
      "context: [70 70 59] target: 1\n",
      "context: [70 70 59  1] target: 71\n",
      "context: [70 70 59  1 71] target: 60\n",
      "context: [70 70 59  1 71 60] target: 70\n",
      "context: [70 70 59  1 71 60 70] target: 71\n",
      "context: [70 70 59  1 71 60 70 71] target: 67\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(44)\n",
    "batch_size = 32\n",
    "# Chunksize = context length, number of tokens to be considered\n",
    "# chunk_size = 8\n",
    "\n",
    "# get training batch\n",
    "# tf.function because the global seed is set and operation seeds are not set\n",
    "# @tf.function\n",
    "# However, adding the tf.function decorator causes the following error:\n",
    "# OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
    "def get_batch(dat):\n",
    "    dat = xtrain if dat == 'train' else val_data\n",
    "    randint = tf.random.uniform(shape=(batch_size,), maxval=len(dat)-chunk_size, dtype=tf.int64)\n",
    "    x = tf.stack([dat[i:i+chunk_size] for i in randint])\n",
    "    y = tf.stack([dat[i+1:i+chunk_size+1] for i in randint])\n",
    "    print(f\"randint: {randint}, randint.shape: {randint.shape}\")\n",
    "    return x,y\n",
    "\n",
    "xs, ys = get_batch('train')\n",
    "# print('inputs:')\n",
    "# print(xs.shape)\n",
    "# print(xs)\n",
    "# print('targets:')\n",
    "# print(ys.shape)\n",
    "# print(ys)\n",
    "\n",
    "\"\"\"\n",
    "x=[1,60,69..10]\n",
    "y = [60,69,..1]\n",
    "input x[0,0:2]= [1,60] output is y[2] = 69\n",
    "\"\"\"\n",
    "\n",
    "print('========================')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for k in range(chunk_size):\n",
    "        context = xs[b,:k+1]\n",
    "        target = ys[b,k]\n",
    "        print(f'context: {context} target: {target}')\n",
    "\n",
    "# xs.shape,ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.no_grad() ~= tf.stop_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 93)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"ngram_lang_model_9\" (type ngramLangModel).\n\nlogits and labels must be broadcastable: logits_size=[256,93] labels_size=[1,256] [Op:SoftmaxCrossEntropyWithLogits]\n\nCall arguments received by layer \"ngram_lang_model_9\" (type ngramLangModel):\n  • xss=tf.Tensor(shape=(32, 8), dtype=int32)\n  • targets=tf.Tensor(shape=(32, 8), dtype=int32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32md:\\.codeEnv\\neitzscheyevskiGPT\\nietzscheyevskiGPT.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/.codeEnv/neitzscheyevskiGPT/nietzscheyevskiGPT.ipynb#X12sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mconvert_to_tensor(Lambda(\u001b[39mlambda\u001b[39;00m x: [x[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m x[:,\u001b[39m1\u001b[39m:] \u001b[39m*\u001b[39m vocab_size])(data))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/.codeEnv/neitzscheyevskiGPT/nietzscheyevskiGPT.ipynb#X12sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m ngram \u001b[39m=\u001b[39m ngramLangModel(vocab_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/.codeEnv/neitzscheyevskiGPT/nietzscheyevskiGPT.ipynb#X12sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m ngram(xs,ys)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;32md:\\.codeEnv\\neitzscheyevskiGPT\\nietzscheyevskiGPT.ipynb Cell 10\u001b[0m in \u001b[0;36mngramLangModel.call\u001b[1;34m(self, xss, targets)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/.codeEnv/neitzscheyevskiGPT/nietzscheyevskiGPT.ipynb#X12sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m logits \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(logits, (B\u001b[39m*\u001b[39mT, C))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/.codeEnv/neitzscheyevskiGPT/nietzscheyevskiGPT.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m targets \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(targets, (B\u001b[39m*\u001b[39mT))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/.codeEnv/neitzscheyevskiGPT/nietzscheyevskiGPT.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49msoftmax_cross_entropy_with_logits(logits\u001b[39m=\u001b[39;49mlogits, labels\u001b[39m=\u001b[39;49mtargets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/.codeEnv/neitzscheyevskiGPT/nietzscheyevskiGPT.ipynb#X12sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_loss(loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/.codeEnv/neitzscheyevskiGPT/nietzscheyevskiGPT.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits,loss\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"ngram_lang_model_9\" (type ngramLangModel).\n\nlogits and labels must be broadcastable: logits_size=[256,93] labels_size=[1,256] [Op:SoftmaxCrossEntropyWithLogits]\n\nCall arguments received by layer \"ngram_lang_model_9\" (type ngramLangModel):\n  • xss=tf.Tensor(shape=(32, 8), dtype=int32)\n  • targets=tf.Tensor(shape=(32, 8), dtype=int32)"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "# from keras import layers, backend\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "# import keras.backend as K\n",
    "# rc: https://www.youtube.com/watch?v=PaCmpygFfXo&t=182s -> Bigram model details by Karpathy\n",
    "\n",
    "# N-gram language model https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "class ngramLangModel(keras.layers.Layer):\n",
    "    def __init__(self, vocab_size):\n",
    "        # `super` call to inherit from keras.layers.Layer\n",
    "        super(ngramLangModel,self).__init__()\n",
    "\n",
    "        self.token_embedding = keras.layers.Embedding(vocab_size, vocab_size)\n",
    "    def build(self, input_shape):\n",
    "        # if subclassers need a \"state creation step\"\n",
    "        # This method is used to create weights that depend on input.shape\n",
    "        # __call__ will auto build the layer if it hasn't been built yet\n",
    "\n",
    "        pass\n",
    "\n",
    "# ?keras.layers.MultiHeadAttention\n",
    "\n",
    "    # !inference mode vs training mode\n",
    "    def call(self, xss, targets=None):\n",
    "        # Called in __call__, after build has been called?\n",
    "        # __call__ wraps call\n",
    "        # Preforms logic of applying the layer to the input tensors\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        logits = self.token_embedding(xss)\n",
    "        print(logits.shape)\n",
    "\n",
    "        # from logits to normalized probabilities\n",
    "        # CategoricalCrossentropy expects labels to be provided in a one hot rep, labels as ints use SparseCategoricalCrossentropy\n",
    "\n",
    "        \"\"\"\n",
    "        # Figure out dimensions\n",
    "        # the err thrown is \"EagerTensor object has no attribute 'reshape'.\"\n",
    "        # the above enabling of numpy behavior has fixed this\n",
    "        \"\"\"\n",
    "\n",
    "        # loss = tf.nn.softmax_cross_entropy_with_logits\n",
    "        # loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        B, T, C = logits.shape\n",
    "        logits = tf.reshape(logits, (B*T, C))\n",
    "        targets = tf.reshape(targets, (B*T))\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "        self.add_loss(loss)\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self, xss, max_new_toks):\n",
    "\n",
    "        for _ in range(max_new_toks):\n",
    "            logits, loss = self(xss)\n",
    "            logits = logits[:, -1, :]\n",
    "            # obtain probability of each token (in this case char)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            next_token = tf.random.categorical(probs, num_samples=1) #(B, 1)\n",
    "            # first dimension/axis is the time dimension\n",
    "            xss = tf.concat([xss, next_token], axis=1)\n",
    "\n",
    "        return xss\n",
    "\n",
    "    # The below func can just be used as a layer. Src: https://blog.codecentric.de/move-n-gram-extraction-into-your-keras-model\n",
    "    # def ngram_block(n, alphabet_size):\n",
    "    #     def wrapped(inputs):\n",
    "    #         layer = layers.Conv1D(1, n, use_bias=False, trainable=False)\n",
    "    #         x = layers.Reshape((-1, 1))(inputs)\n",
    "    #         x = layer(x)\n",
    "    #         kernel = np.power(alphabet_size, range(0, n),\n",
    "    #                           dtype=backend.floatx())\n",
    "    #         layer.set_weights([kernel.reshape(n, 1, 1)])\n",
    "    #         return layers.Reshape((-1,))(x)\n",
    "\n",
    "    #     return wrapped\n",
    "\n",
    "    def bigram(self, data):\n",
    "        return tf.convert_to_tensor(Lambda(lambda x: [x[:,:-1] + x[:,1:] * vocab_size])(data))\n",
    "\n",
    "\n",
    "\n",
    "ngram = ngramLangModel(vocab_size)\n",
    "\n",
    "# ys.shape,xs.shape\n",
    "# out,loss = ngram(xs,ys)\n",
    "# init_xss = tf.zeros((1, 1), dtype=tf.int64)\n",
    "# out = tf.nn.softmax(out)\n",
    "# x = out[0].numpy().tolist()\n",
    "# decode(x)\n",
    "# ngram.generate(init_xss, max_new_toks=10)\n",
    "# print(outt)\n",
    "# print(loss.call(ys,out))\n",
    "# type(bgramd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on gpu\n",
    "# with tf.device('/GPU:0'):\n",
    "#     model = get_model()\n",
    "#     model.compile()\n",
    "#     model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
